{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_spacy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMVpWtcGRv/xvmKI9riOyEW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipanshuhaldar/Custom_NER_SpaCy/blob/master/NER_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmW62EgFAnO5",
        "colab_type": "text"
      },
      "source": [
        "#Build a Custom NER Model with SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvsH8-WHAhzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#install SpaCy\n",
        "!pip install -U spacy\n",
        "!python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rrwkMXUCO5P",
        "colab_type": "code",
        "outputId": "9e24ec8d-a75e-426e-ea2e-a554341e5fcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "#mounting google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtdiFna9FryX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/entity_annotated_corpus')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAhMCWfGGFDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZx3MMY0GFvM",
        "colab_type": "code",
        "outputId": "be960c36-a0f8-4e1e-c4f2-6f843a460b15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "ner_data = pd.read_csv('ner_dataset.csv',engine = 'python')\n",
        "ner_data.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sentence: 1</td>\n",
              "      <td>Thousands</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>of</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>demonstrators</td>\n",
              "      <td>NNS</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>have</td>\n",
              "      <td>VBP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>marched</td>\n",
              "      <td>VBN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Sentence #           Word  POS Tag\n",
              "0  Sentence: 1      Thousands  NNS   O\n",
              "1          NaN             of   IN   O\n",
              "2          NaN  demonstrators  NNS   O\n",
              "3          NaN           have  VBP   O\n",
              "4          NaN        marched  VBN   O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkwbrqI1z4K5",
        "colab_type": "code",
        "outputId": "4c024121-34f8-4006-9255-d0cea67db469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "row_num = ner_data.loc[ner_data['Sentence #']=='Sentence: 261'].index.values[0] - 1\n",
        "ner_data = ner_data.head(row_num)\n",
        "ner_data.tail()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence #</th>\n",
              "      <th>Word</th>\n",
              "      <th>POS</th>\n",
              "      <th>Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5717</th>\n",
              "      <td>NaN</td>\n",
              "      <td>from</td>\n",
              "      <td>IN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5718</th>\n",
              "      <td>NaN</td>\n",
              "      <td>the</td>\n",
              "      <td>DT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5719</th>\n",
              "      <td>NaN</td>\n",
              "      <td>oil-for-food</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5720</th>\n",
              "      <td>NaN</td>\n",
              "      <td>program</td>\n",
              "      <td>NN</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5721</th>\n",
              "      <td>NaN</td>\n",
              "      <td>itself</td>\n",
              "      <td>PRP</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     Sentence #          Word  POS Tag\n",
              "5717        NaN          from   IN   O\n",
              "5718        NaN           the   DT   O\n",
              "5719        NaN  oil-for-food   NN   O\n",
              "5720        NaN       program   NN   O\n",
              "5721        NaN        itself  PRP   O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yBZHRfDGwpL",
        "colab_type": "code",
        "outputId": "554dd823-55fc-400b-9e78-a339b5d50554",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#deleting Sentence # column \n",
        "del ner_data['Sentence #']\n",
        "del ner_data['POS']\n",
        "ner_data.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Word', 'Tag'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uesuTqnFYW2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indexNames = ner_data[ner_data['Word'] == '\"'].index\n",
        "# Delete these row indexes from dataFrame\n",
        "ner_data.drop(indexNames , inplace=True)\n",
        "\n",
        "#indexNames = ner_data[ner_data['Word'] == '10,000'].index\n",
        "# Delete these row indexes from dataFrame\n",
        "#ner_data.drop(indexNames , inplace=True)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh2Y3h85z4XF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ner_data.to_csv('ner_corpus.tsv', sep = '\\t', header = False, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1LR7hCUz305",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert .tsv file to dataturks json format. \n",
        "import json\n",
        "import logging\n",
        "import sys\n",
        "def tsv_to_json_format(input_path,output_path,unknown_label):\n",
        "    try:\n",
        "        f=open(input_path,'r') # input file\n",
        "        fp=open(output_path, 'w') # output file\n",
        "        data_dict={}\n",
        "        annotations =[]\n",
        "        label_dict={}\n",
        "        s=''\n",
        "        start=0\n",
        "        for line in f:\n",
        "            if line[0:len(line)-1]!='.\\tO':\n",
        "                word,entity=line.split('\\t')\n",
        "                s+=word+\" \"\n",
        "                entity=entity[:len(entity)-1]\n",
        "                if entity!=unknown_label:\n",
        "                    if len(entity) != 1:\n",
        "                        d={}\n",
        "                        d['text']=word\n",
        "                        d['start']=start\n",
        "                        d['end']=start+len(word)-1  \n",
        "                        try:\n",
        "                            label_dict[entity].append(d)\n",
        "                        except:\n",
        "                            label_dict[entity]=[]\n",
        "                            label_dict[entity].append(d) \n",
        "                start+=len(word)+1\n",
        "            else:\n",
        "                data_dict['content']=s\n",
        "                s=''\n",
        "                label_list=[]\n",
        "                for ents in list(label_dict.keys()):\n",
        "                    for i in range(len(label_dict[ents])):\n",
        "                        if(label_dict[ents][i]['text']!=''):\n",
        "                            l=[ents,label_dict[ents][i]]\n",
        "                            for j in range(i+1,len(label_dict[ents])): \n",
        "                                if(label_dict[ents][i]['text']==label_dict[ents][j]['text']):  \n",
        "                                    di={}\n",
        "                                    di['start']=label_dict[ents][j]['start']\n",
        "                                    di['end']=label_dict[ents][j]['end']\n",
        "                                    di['text']=label_dict[ents][i]['text']\n",
        "                                    l.append(di)\n",
        "                                    label_dict[ents][j]['text']=''\n",
        "                            label_list.append(l)                          \n",
        "                            \n",
        "                for entities in label_list:\n",
        "                    label={}\n",
        "                    label['label']=[entities[0]]\n",
        "                    label['points']=entities[1:]\n",
        "                    annotations.append(label)\n",
        "                data_dict['annotation']=annotations\n",
        "                annotations=[]\n",
        "                json.dump(data_dict, fp)\n",
        "                fp.write('\\n')\n",
        "                data_dict={}\n",
        "                start=0\n",
        "                label_dict={}\n",
        "    except Exception as e:\n",
        "        logging.exception(\"Unable to process file\" + \"\\n\" + \"error = \" + str(e))\n",
        "        return None\n",
        "\n",
        "tsv_to_json_format(\"ner_corpus.tsv\",'ner_corpus_260.json','abc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZCBgbY2QiJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert json file to spaCy format.\n",
        "import plac\n",
        "import logging\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "input_file = 'ner_corpus_260.json'\n",
        "output_file = 'ner_corpus_260_custom'\n",
        "\n",
        "training_data = []\n",
        "lines=[]\n",
        "with open(input_file, 'r') as f:\n",
        "  lines = f.readlines()\n",
        "for line in lines:\n",
        "  data = json.loads(line)\n",
        "  text = data['content']\n",
        "  entities = []  \n",
        "\n",
        "for line in lines:\n",
        "  data = json.loads(line)\n",
        "  text = data['content']\n",
        "  entities = [] \n",
        "  for annotation in data['annotation']:\n",
        "    point = annotation['points'][0]\n",
        "    labels = annotation['label']\n",
        "    if not isinstance(labels, list):\n",
        "      labels = [labels] \n",
        "\n",
        "    for label in labels:\n",
        "      entities.append((point['start'], point['end'] + 1 ,label))  \n",
        "\n",
        "  training_data.append((text, {\"entities\" : entities}))\n",
        "\n",
        "#print(training_data) \n",
        "\n",
        "with open(output_file, 'wb') as fp:\n",
        "  pickle.dump(training_data, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZUpQ_c1Vcuq",
        "colab_type": "code",
        "outputId": "39984d8b-1a20-4a10-bbc2-a381dfceb77c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training additional entity types using spaCy\n",
        "from __future__ import unicode_literals, print_function\n",
        "import pickle\n",
        "import plac\n",
        "import random\n",
        "from pathlib import Path\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "LABEL = ['I-geo', 'B-geo', 'I-art', 'B-art', 'B-tim', 'B-nat', 'B-eve', 'O', 'I-per', 'I-tim', 'I-nat', 'I-eve', 'B-per', 'I-org', 'B-gpe', 'B-org', 'I-gpe']\n",
        "\n",
        "\"\"\"\n",
        "geo = Geographical Entity\n",
        "org = Organization\n",
        "per = Person\n",
        "gpe = Geopolitical Entity\n",
        "tim = Time indicator\n",
        "art = Artifact\n",
        "eve = Event\n",
        "nat = Natural Phenomenon\n",
        "\"\"\"\n",
        "\n",
        "# Loading training data \n",
        "with open ('ner_corpus_260_custom', 'rb') as fp:\n",
        "    TRAIN_DATA = pickle.load(fp)\n",
        "\n",
        "model= 'en'\n",
        "new_model_name= 'new_model'\n",
        "output_dir= '/content/drive/My Drive/entity_annotated_corpus'\n",
        "n_iter= 1000  \n",
        "\n",
        "\"\"\"Setting up the pipeline and entity recognizer, and training the new entity.\"\"\"\n",
        "\n",
        "if model is not None:\n",
        "  nlp = spacy.load(model)  # load existing spacy model\n",
        "  print(\"Loaded model '%s'\" % model)\n",
        "else:\n",
        "  nlp = spacy.blank('en')  # create blank Language class\n",
        "  print(\"Created blank 'en' model\")\n",
        "\n",
        "if 'ner' not in nlp.pipe_names:\n",
        "  ner = nlp.create_pipe('ner')\n",
        "  nlp.add_pipe(ner)\n",
        "else:\n",
        "  ner = nlp.get_pipe('ner')\n",
        "\n",
        "for i in LABEL:\n",
        "        ner.add_label(i)   # Add new entity labels to entity recognizer\n",
        "\n",
        "if model is None:\n",
        "  optimizer = nlp.begin_training()\n",
        "else:\n",
        "  optimizer = nlp.entity.create_optimizer()\n",
        "\n",
        "# Get names of other pipes to disable them during training to train only NER\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "  for itn in range(n_iter):\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    losses = {}\n",
        "    batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
        "    for batch in batches:\n",
        "      texts, annotations = zip(*batch)\n",
        "      nlp.update(texts, annotations, sgd=optimizer, drop=0.35,losses=losses)\n",
        "    print('Losses', losses) \n",
        "\n",
        "# Test the trained model\n",
        "test_text = 'Gianni Infantino is the president of FIFA.'\n",
        "doc = nlp(test_text)\n",
        "print(\"Entities in '%s'\" % test_text)\n",
        "for ent in doc.ents:\n",
        "  print(ent.label_, ent.text)      \n",
        "\n",
        "\n",
        "# Save model \n",
        "if output_dir is not None:\n",
        "  output_dir = Path(output_dir)\n",
        "  if not output_dir.exists():\n",
        "    output_dir.mkdir()\n",
        "  nlp.meta['name'] = new_model_name  # rename model\n",
        "  nlp.to_disk(output_dir)\n",
        "  print(\"Saved model to\", output_dir)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded model 'en'\n",
            "Losses {'ner': 3578.391148540133}\n",
            "Losses {'ner': 3231.3551100492477}\n",
            "Losses {'ner': 3354.977543488145}\n",
            "Losses {'ner': 3305.136632978916}\n",
            "Losses {'ner': 3236.1561207324266}\n",
            "Losses {'ner': 3135.595994628966}\n",
            "Losses {'ner': 3203.8640112280846}\n",
            "Losses {'ner': 3091.083201613277}\n",
            "Losses {'ner': 3012.4705471247435}\n",
            "Losses {'ner': 3116.1846166397445}\n",
            "Losses {'ner': 3053.858446510043}\n",
            "Losses {'ner': 3033.2174602150917}\n",
            "Losses {'ner': 2973.7969525828958}\n",
            "Losses {'ner': 3004.1665944457054}\n",
            "Losses {'ner': 2975.7895978973247}\n",
            "Losses {'ner': 2954.7073140200227}\n",
            "Losses {'ner': 2925.4500223414507}\n",
            "Losses {'ner': 3007.6629916590173}\n",
            "Losses {'ner': 3011.349388629198}\n",
            "Losses {'ner': 2926.2706830725074}\n",
            "Losses {'ner': 3034.8907467344543}\n",
            "Losses {'ner': 2955.21490319242}\n",
            "Losses {'ner': 2944.2207369743846}\n",
            "Losses {'ner': 2911.5105652809143}\n",
            "Losses {'ner': 2968.797410971485}\n",
            "Losses {'ner': 2981.6036707796156}\n",
            "Losses {'ner': 3009.7748352885246}\n",
            "Losses {'ner': 3007.826277717948}\n",
            "Losses {'ner': 2947.154062759131}\n",
            "Losses {'ner': 2979.8281358256936}\n",
            "Losses {'ner': 2963.3298964984715}\n",
            "Losses {'ner': 2926.5110105704516}\n",
            "Losses {'ner': 2952.9392100110417}\n",
            "Losses {'ner': 2881.914558842778}\n",
            "Losses {'ner': 2921.772706411779}\n",
            "Losses {'ner': 2917.7518273058813}\n",
            "Losses {'ner': 2942.652493732632}\n",
            "Losses {'ner': 2881.0508719086647}\n",
            "Losses {'ner': 2920.5469936523587}\n",
            "Losses {'ner': 2969.1226806640625}\n",
            "Losses {'ner': 2905.959172035102}\n",
            "Losses {'ner': 2936.309688395355}\n",
            "Losses {'ner': 2865.714967914915}\n",
            "Losses {'ner': 3020.834783813916}\n",
            "Losses {'ner': 2902.585526566254}\n",
            "Losses {'ner': 2968.315262082964}\n",
            "Losses {'ner': 2964.127102114493}\n",
            "Losses {'ner': 2926.8287222594954}\n",
            "Losses {'ner': 2857.872737633297}\n",
            "Losses {'ner': 2946.932172216475}\n",
            "Losses {'ner': 2906.312107356265}\n",
            "Losses {'ner': 2942.5075547001325}\n",
            "Losses {'ner': 2854.060176990344}\n",
            "Losses {'ner': 2834.445499924943}\n",
            "Losses {'ner': 2891.374968881428}\n",
            "Losses {'ner': 2845.066461276263}\n",
            "Losses {'ner': 2919.901959732175}\n",
            "Losses {'ner': 2781.2971685023986}\n",
            "Losses {'ner': 2948.617823845707}\n",
            "Losses {'ner': 2832.1124987705844}\n",
            "Losses {'ner': 2887.6158141249325}\n",
            "Losses {'ner': 2881.2908359914877}\n",
            "Losses {'ner': 2880.7634119586}\n",
            "Losses {'ner': 2893.5813287496567}\n",
            "Losses {'ner': 2788.1727680936456}\n",
            "Losses {'ner': 2892.892144071375}\n",
            "Losses {'ner': 2917.606578881736}\n",
            "Losses {'ner': 2804.8936925157905}\n",
            "Losses {'ner': 2931.5645242713435}\n",
            "Losses {'ner': 2892.170354493428}\n",
            "Losses {'ner': 2891.5541000692174}\n",
            "Losses {'ner': 2900.0674879178405}\n",
            "Losses {'ner': 2852.454239508137}\n",
            "Losses {'ner': 2933.5091513091465}\n",
            "Losses {'ner': 2794.6199205869343}\n",
            "Losses {'ner': 2890.5903956803377}\n",
            "Losses {'ner': 2871.591502469033}\n",
            "Losses {'ner': 2869.242343406193}\n",
            "Losses {'ner': 2876.9049361668294}\n",
            "Losses {'ner': 2955.793385422352}\n",
            "Losses {'ner': 2877.3580685704947}\n",
            "Losses {'ner': 2898.4879448570427}\n",
            "Losses {'ner': 2838.888789512217}\n",
            "Losses {'ner': 3025.6409968957305}\n",
            "Losses {'ner': 2997.277943611145}\n",
            "Losses {'ner': 2975.545988358557}\n",
            "Losses {'ner': 2939.5843337997794}\n",
            "Losses {'ner': 2892.1082723863074}\n",
            "Losses {'ner': 2854.4746273513883}\n",
            "Losses {'ner': 2847.4209582379553}\n",
            "Losses {'ner': 2931.6667350307107}\n",
            "Losses {'ner': 2962.082946209237}\n",
            "Losses {'ner': 2901.518215786142}\n",
            "Losses {'ner': 2951.8233098804485}\n",
            "Losses {'ner': 2946.898220913048}\n",
            "Losses {'ner': 2832.5561291315244}\n",
            "Losses {'ner': 2950.112536726927}\n",
            "Losses {'ner': 2879.3475313099334}\n",
            "Losses {'ner': 2850.4347947873175}\n",
            "Losses {'ner': 2902.592507065856}\n",
            "Entities in 'Gianni Infantino is the president of FIFA.'\n",
            "B-per Gianni\n",
            "I-per Infantino\n",
            "B-org FIFA\n",
            "Saved model to /content/drive/My Drive/entity_annotated_corpus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6WKHzS6UgRG",
        "colab_type": "code",
        "outputId": "ff1a6b3a-f50c-473a-e665-84aca0234b7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "output_dir= '/content/drive/My Drive/entity_annotated_corpus'\n",
        "\n",
        "import spacy\n",
        "from spacy.util import minibatch, compounding\n",
        "\n",
        "# Test the saved model\n",
        "print(\"Loading from\", output_dir)\n",
        "nlp2 = spacy.load(output_dir)\n",
        "#test_text = 'John Lee is the chief of CBSE'\n",
        "test_text = 'Americans suffered from H5N1'\n",
        "\n",
        "doc2 = nlp2(test_text)\n",
        "for ent in doc2.ents:\n",
        "  print(ent.label_, ent.text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading from /content/drive/My Drive/entity_annotated_corpus\n",
            "B-gpe Americans\n",
            "B-tim H5N1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c61BAQ_FVgoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}